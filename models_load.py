import os
import sys
import requests
import json
# import re

from config import *
from sqlite_helper import *
# from mssql_helper import *
import do_history


# ollamaæ¨¡å‹
# from langchain_community.embeddings import OllamaEmbeddings # é‡åŒ–æ–‡æ¡£
# from langchain_community.llms import Ollama #æ¨¡å‹
from langchain_ollama import OllamaEmbeddings, ChatOllama

# cohereé‡æ’æ¨¡å‹
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_community.llms import Cohere

# geminiæ¨¡å‹
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# æç¤ºè¯æ¨¡æ¿
from langchain_core.prompts import ChatPromptTemplate

# é€šä¹‰åƒé—®æ¨¡å‹
from langchain_community.llms import Tongyi
import dashscope
# deepseek æ¨¡å‹
from langchain_deepseek import ChatDeepSeek # pip install -qU langchain-deepseek
from langchain_openai import ChatOpenAI

# è¯­ä¹‰æ£€ç´¢
from langchain.schema.runnable import RunnableMap
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# chatGLM3-6B æ¨¡å‹
from langchain_community.llms.chatglm3 import ChatGLM3


# kimi æ¨¡å‹
from langchain_community.llms.moonshot import Moonshot

# groq api æ¨¡å‹
from langchain_groq import ChatGroq

# deepseek æ¨¡å‹
from langchain_deepseek import ChatDeepSeek # pip install -qU langchain-deepseek

# chatGLM3-6B æ¨¡å‹
from langchain_community.llms.chatglm3 import ChatGLM3

# å¼‚æ­¥å‡½æ•°
import asyncio


# è¯»å–ç¯å¢ƒå˜é‡
from dotenv import load_dotenv #pip install python-dotenv

# è¯»å– .env æ–‡ä»¶
load_dotenv()

############################# API KEY #################################
# å°†å„ä¸ªåœ¨çº¿æ¨¡å‹ API key åŠ å…¥ç¯å¢ƒå˜é‡
# os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY
# os.environ['DASHSCOPE_API_KEY'] = DASHSCOPE_API_KEY
# os.environ["MOONSHOT_API_KEY"] = MOONSHOT_API_KEY
# os.environ["GROQ_API_KEY"] = GROQ_API_KEY
# os.environ["COHERE_API_KEY"] = COHERE_API_KEY
############################# é‡åŒ–æ¨¡å‹ #################################
# æœ¬åœ°é‡åŒ–æ¨¡å‹
embedding_ollama = OllamaEmbeddings(
    base_url = embedding_ollama_conf["base_url"], 
    model = embedding_ollama_conf["model"]
) 

############################# è¯­è¨€æ¨¡å‹ #################################
# æœ¬åœ°è¯­è¨€æ¨¡å‹
# llm_ollama = Ollama(
#     base_url = llm_ollama_conf["base_url"], 
#     model = llm_ollama_conf["model"]
# )

llm_ollama = ChatOllama(
    base_url = llm_ollama_conf["base_url"], 
    model = llm_ollama_conf["model"],
    temperature = llm_ollama_conf["temperature"],
    num_predict = llm_ollama_conf["num_predict"]
)

# åœ¨çº¿è¯­è¨€æ¨¡å‹ gemini
llm_gemini = ChatGoogleGenerativeAI(
    model = llm_gemini_conf["model"],
    temperature = llm_gemini_conf["temperature"]
) 
# çº¿ä¸Šgoogleé‡åŒ–æ¨¡å‹
embedding_google = GoogleGenerativeAIEmbeddings(
    model = embedding_google_conf["model"]
) 

# # åœ¨çº¿è¯­è¨€æ¨¡å‹ é€šä¹‰åƒé—®
# llm_tongyi = Tongyi(
#     model_name = llm_tongyi_conf["model_name"],
#     temperature = llm_tongyi_conf["temperature"],
#     streaming = llm_tongyi_conf["streaming"]
#     #enable_search = True
# ) 

# æ›´æ–°
llm_tongyi = ChatOpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model=llm_tongyi_conf["model_name"],  # æ­¤å¤„ä»¥qwen-plusä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models
    # other params...
)



# åœ¨çº¿è¯­è¨€æ¨¡å‹ kimi
llm_kimi = Moonshot(
    model_name = llm_kimi_conf["model_name"],
    temperature = llm_kimi_conf["temperature"]
) 
# åœ¨çº¿è¯­è¨€æ¨¡å‹ groq
llm_groq = ChatGroq(
    model_name = llm_groq_conf["model_name"],
    temperature = llm_groq_conf["temperature"]
)
# åœ¨çº¿è¯­è¨€æ¨¡å‹ deepseek
llm_deepseek = ChatDeepSeek(
    model = llm_deepseek_conf["model"],
    temperature = llm_deepseek_conf["temperature"],
    max_tokens = llm_deepseek_conf["max_tokens"],
    timeout = llm_deepseek_conf["timeout"],
    max_retries = llm_deepseek_conf["max_retries"]
) 

# æœ¬åœ°è¯­è¨€æ¨¡å‹ ChatGLM3
llm_chatGLM = ChatGLM3(
    endpoint_url = llm_chatGLM_conf["endpoint_url"],
    max_tokens = llm_chatGLM_conf["max_tokens"],
    top_p = llm_chatGLM_conf["top_p"]
) 


############################# æ¨¡å‹é€‰æ‹© #################################
# è¯»å–æ•°æ®åº“
def get_models_on_request():
    models = get_models_table()
    must_use_llm_rag = models["must_use_llm_rag"]
    # é€‰æ‹©é‡åŒ–æ¨¡å‹
    if models["embedding"] == "ollama":
        embedding = embedding_ollama
    else:
        embedding = embedding_google

    # é€‰æ‹©èŠå¤©è¯­è¨€æ¨¡å‹
    if models["llm"] == "ollama":
        llm = llm_ollama
    elif models["llm"] == "gemini": 
        llm = llm_gemini
    elif models["llm"] == "tongyi": 
        llm = llm_tongyi
    elif models["llm"] == "kimi": 
        llm = llm_kimi
    elif models["llm"] == "groq": 
        llm = llm_groq
    else:
        llm = llm_chatGLM

    # é€‰æ‹©çŸ¥è¯†åº“è¯­è¨€æ¨¡å‹
    if models["llm_rag"] == "ollama":
        llm_rag = llm_ollama
    elif models["llm_rag"] == "gemini": 
        llm_rag = llm_gemini
    elif models["llm_rag"] == "tongyi": 
        llm_rag = llm_tongyi
    elif models["llm_rag"] == "kimi": 
        llm_rag = llm_kimi
    elif models["llm_rag"] == "groq": 
        llm_rag = llm_groq
    else:
        llm_rag = llm_chatGLM
    # llm_rag.temperature = 0.0
    return embedding, llm, llm_rag, must_use_llm_rag
    
############################# æ¨¡å‹æ–¹æ³• #################################

# # å»é™¤é‡å¤çš„æ˜µç§°
# def remove_repeated_nicknames(content, nick_name):
#     # ä½¿ç”¨æ ¼å¼åŒ–å­—ç¬¦ä¸²å°†å˜é‡åµŒå…¥åˆ°æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ä¸­
#     pattern = rf'({re.escape(nick_name)})(?:\1)+'
#     # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼Œä¿ç•™ä¸€ä¸ª user_nick_name
#     result = re.sub(pattern, r'\1', content)
#     return result



# å‘é‡æ£€ç´¢èŠå¤©ï¼ˆæ‰§è¡Œå‘é‡é“¾ï¼‰
async def run_chain(bot_nick_name, user_nick_name, retriever, source_id, query, user_state="èŠå¤©", name_space="test", template_string=""):
    # æ˜¯å¦ä½¿ç”¨é‡æ’æ¨¡å‹
    if must_rerank_embedding == 1:
        # llm = Cohere(temperature=0)
        # å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’
        compressor = CohereRerank() 
        retriever = ContextualCompressionRetriever(
            base_compressor = compressor, 
            base_retriever = retriever # æœªé‡æ’å‰çš„æ£€ç´¢ç»“æœ
        )
        
    embedding, llm, llm_rag, must_use_llm_rag = get_models_on_request()

    if query !="" and query is not None:
        print("=" * 50)
        print("å½“å‰ä½¿ç”¨çš„çŸ¥è¯†åº“LLMï¼š", llm_rag)
        if template_string == "":
            template_string = f"""ä½ çš„åå­—å«{bot_nick_name}ï¼Œè¯·æ ¹æ®æ–‡æ¡£å†…å®¹ç”¨ç®€ä½“ä¸­æ–‡å®Œæ•´åœ°å›ç­”é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿå–„äºä»å¤§å®¶å¤æ‚çš„å¯¹è¯ä¸­åˆ†ææ¯ä¸ªäººè¡¨è¾¾çš„æ„å›¾ï¼Œä»¥ä¸‹æ˜¯æˆ‘å¿…é¡»æé†’ä½ æ³¨æ„çš„ï¼š
            
## æ³¨æ„åˆ†æ¸…æ¥šæ˜¯è°å¯¹è°è¯´è¯
- å½“æ¶ˆæ¯åŒ…å«æœ‰ç±»ä¼¼äºâ€œ@ç¾¤æˆå‘˜â€, å¹¶ä¸”ä¸æ˜¯â€œ@{bot_nick_name}â€ï¼Œå°±è¡¨ç¤ºç”¨æˆ·åœ¨å’Œå¦å¤–ä¸€ä¸ªç”¨æˆ·è¯´è¯ã€‚
- å¦‚æœæ¶ˆæ¯ä¸­åŒ…å«æœ‰â€œ@{bot_nick_name}â€è¿™æ ·çš„å­—ç¬¦ï¼Œè¡¨æ˜ç”¨æˆ·æ˜¯å¯¹ä½ è¯´ï¼Œè¯·ä½ å›ç­”æ—¶åœ¨å†…å®¹å‰åŠ ä¸Šâ€œ@{user_nick_name} â€ã€‚
- å¦‚æœä½ çš„å›ç­”æ˜¯é’ˆå¯¹æŸä¸ªç”¨æˆ·çš„ï¼Œè¯·åœ¨å›ç­”çš„å†…å®¹å‰é¢åŠ ä¸Šâ€œ@ç”¨æˆ· â€ï¼Œè¿™æ ·ç”¨æˆ·ä¼šçœ‹åˆ°ä½ çš„å›ç­”æ˜¯é’ˆå¯¹ä»–ã€‚
- è¯·ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚Please output in Chinese. """ # é»˜è®¤æ¨¡æ¿
        
        template_cn = """%s
        {context}
        {question}
        """ % template_string
        
        

        # å¤„ç†èŠå¤©è®°å½•
        chat_history = do_history.get_chat_history(source_id, user_state, name_space, bot_nick_name, query)
            
            
        # ç”±æ¨¡æ¿ç”Ÿæˆprompt
        prompt = ChatPromptTemplate.from_template(template_cn) 

        # åˆ›å»ºchain
        chain = RunnableMap({
            "context": lambda x: retriever.invoke(x),
            "question": RunnablePassthrough(),
            "chat_history": lambda x: chat_history  # ä½¿ç”¨å†å²è®°å½•çš„æ­¥éª¤
        }) | prompt | llm_rag | StrOutputParser()
        
        # æ‰§è¡Œé—®ç­”
        request = f'{{"user":"{user_nick_name}", "content":"{query}"}}'
        try:
            response_message = chain.invoke(request)
        except Exception as e:
            response_message = f"LLMå“åº”é”™è¯¯: {e}"
            print(f"LLMå“åº”é”™è¯¯: {e}")
            
        # è¿”å›ç»“æœ
        return response_message + "ğŸ˜Š"



# from langchain.chains import RetrievalQA
# async def run_chain(bot_nick_name, user_nick_name, retriever, source_id, query, user_state="èŠå¤©", name_space="test", template_string=""):
#     if must_rerank_embedding == 1:
#         compressor = CohereRerank()
#         retriever = ContextualCompressionRetriever(
#             base_compressor=compressor,
#             base_retriever=retriever
#         )
    
#     embedding, llm, llm_rag, must_use_llm_rag = get_models_on_request()
    
#     if query:
#         print("=" * 50)
#         print("å½“å‰ä½¿ç”¨çš„çŸ¥è¯†åº“LLMï¼š", llm_rag)
        
#         if not template_string:
#             template_string = f"""
#             ä½ çš„åå­—å«{bot_nick_name}ï¼Œè¯·æ ¹æ®æ–‡æ¡£å†…å®¹ç”¨ç®€ä½“ä¸­æ–‡å®Œæ•´åœ°å›ç­”é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿå–„äºä»å¤§å®¶å¤æ‚çš„å¯¹è¯ä¸­åˆ†ææ¯ä¸ªäººè¡¨è¾¾çš„æ„å›¾ï¼Œä»¥ä¸‹æ˜¯æˆ‘å¿…é¡»æé†’ä½ æ³¨æ„çš„ï¼š
            
#             ## æ³¨æ„åˆ†æ¸…æ¥šæ˜¯è°å¯¹è°è¯´è¯
#             - å½“æ¶ˆæ¯åŒ…å«æœ‰ç±»ä¼¼äºâ€œ@ç¾¤æˆå‘˜â€, å¹¶ä¸”ä¸æ˜¯â€œ@{bot_nick_name}â€ï¼Œå°±è¡¨ç¤ºç”¨æˆ·åœ¨å’Œå¦å¤–ä¸€ä¸ªç”¨æˆ·è¯´è¯ã€‚
#             - å¦‚æœæ¶ˆæ¯ä¸­åŒ…å«æœ‰â€œ@{bot_nick_name}â€è¿™æ ·çš„å­—ç¬¦ï¼Œè¡¨æ˜ç”¨æˆ·æ˜¯å¯¹ä½ è¯´ï¼Œè¯·ä½ å›ç­”æ—¶åœ¨å†…å®¹å‰åŠ ä¸Šâ€œ@{user_nick_name} â€ã€‚
#             - å¦‚æœä½ çš„å›ç­”æ˜¯é’ˆå¯¹æŸä¸ªç”¨æˆ·çš„ï¼Œè¯·åœ¨å›ç­”çš„å†…å®¹å‰é¢åŠ ä¸Šâ€œ@ç”¨æˆ· â€ï¼Œè¿™æ ·ç”¨æˆ·ä¼šçœ‹åˆ°ä½ çš„å›ç­”æ˜¯é’ˆå¯¹ä»–ã€‚
#             - è¯·ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚Please output in Chinese.
#             """
        
#         template_cn = f"""
#         {template_string}
#         {{chat_history}}
#         {{context}}
#         {{question}}
#         """
        
#         chat_history = do_history.get_chat_history(source_id, user_state, name_space, bot_nick_name, query)
        
#         chain = RetrievalQA.from_chain_type(llm_rag, retriever=retriever)
        
#         request = f'{{"user":"{user_nick_name}", "content":"{query}", "chat_history": "{chat_history}"}}'
#         try:
#             response_message_dict = chain.invoke({"query": "è¯·ç”¨ä¸­æ–‡å›ç­”æˆ‘ï¼š" + request, "chat_history": chat_history})
#             response_message = response_message_dict.get("result", "").strip()
#         except Exception as e:
#             response_message = f"LLMå“åº”é”™è¯¯: {e}"
#             print(f"LLMå“åº”é”™è¯¯: {e}")
        
#         return response_message + "ğŸ˜Š"


# é€šç”¨èŠå¤©
async def chat_generic_langchain(bot_nick_name, user_nick_name, source_id, query, user_state="èŠå¤©", name_space="test", template_string=""):
    embedding, llm, llm_rag, must_use_llm_rag = get_models_on_request()
    if query !="" and query is not None:

        # å¤„ç†èŠå¤©è®°å½•
        chat_history = do_history.get_chat_history(source_id, user_state, name_space, bot_nick_name, query)
            
        if template_string == "":
            template_string = f"""ä½ æ˜¯ä¸€ä¸ªçƒ­å¿ƒçš„äººï¼Œä½ çš„åå­—å«{bot_nick_name}ï¼Œå°½åŠ›ä¸ºäººä»¬è§£ç­”é—®é¢˜.åŒæ—¶ä¹Ÿå–„äºä»å¤§å®¶å¤æ‚çš„å¯¹è¯ä¸­åˆ†ææ¯ä¸ªäººè¡¨è¾¾çš„æ„å›¾ï¼Œä»¥ä¸‹æ˜¯æˆ‘å¿…é¡»æé†’ä½ æ³¨æ„çš„ï¼š
            
## æ³¨æ„åˆ†æ¸…æ¥šæ˜¯è°å¯¹è°è¯´è¯
- å½“æ¶ˆæ¯åŒ…å«æœ‰ç±»ä¼¼äºâ€œ@ç¾¤æˆå‘˜â€, å¹¶ä¸”ä¸æ˜¯â€œ@{bot_nick_name}â€ï¼Œå°±è¡¨ç¤ºç”¨æˆ·åœ¨å’Œå¦å¤–ä¸€ä¸ªç”¨æˆ·è¯´è¯ã€‚
- å¦‚æœæ¶ˆæ¯ä¸­åŒ…å«æœ‰â€œ@{bot_nick_name}â€è¿™æ ·çš„å­—ç¬¦ï¼Œè¡¨æ˜ç”¨æˆ·æ˜¯å¯¹ä½ è¯´ï¼Œè¯·ä½ å›ç­”æ—¶åœ¨å†…å®¹å‰åŠ ä¸Šâ€œ@{user_nick_name} â€ã€‚
- å¦‚æœä½ çš„å›ç­”æ˜¯é’ˆå¯¹æŸä¸ªç”¨æˆ·çš„ï¼Œè¯·åœ¨å›ç­”çš„å†…å®¹å‰é¢åŠ ä¸Šâ€œ@ç”¨æˆ· â€ï¼Œè¿™æ ·ç”¨æˆ·ä¼šçœ‹åˆ°ä½ çš„å›ç­”æ˜¯é’ˆå¯¹ä»–ã€‚
- è¯·ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚Please output in Chinese. """ # é»˜è®¤æ¨¡æ¿
            
        # ç”±æ¨¡æ¿ç”Ÿæˆ prompt
        prompt = ChatPromptTemplate.from_template("""
            %s
            {chat_history}
            {question}
        """ % template_string)
        print("=" * 50)
        
        # åˆ›å»ºé“¾ï¼Œå°†å†å²è®°å½•ä¼ é€’ç»™é“¾
        if user_state != "èŠå¤©" and must_use_llm_rag == 1:
            chain = {
                "question": RunnablePassthrough(), 
                "chat_history": lambda x: chat_history,
            } | prompt | llm_rag | StrOutputParser()  
            print("å½“å‰ä½¿ç”¨çš„èŠå¤©LLMï¼š", llm_rag)
        else:
            chain = {
                "question": RunnablePassthrough(), 
                "chat_history": lambda x: chat_history,
            } | prompt | llm | StrOutputParser()  
            print("å½“å‰ä½¿ç”¨çš„èŠå¤©LLMï¼š", llm)

        # æ‰§è¡Œé—®ç­”
        request = f'{{"user":"{user_nick_name}", "content":"{query}"}}'
        try:
            response_message = chain.invoke(request)
            # å¤„ç†èŠå¤©è®°å½• 
            # await do_chat_history(chat_history, source_id, user_nick_name, query, user_state, name_space)
            # do_chat_history(chat_history, source_id, bot_nick_name, response_message, user_state, name_space)
        except Exception as e:
            response_message = f"LLMå“åº”é”™è¯¯: {e}"
            print(f"LLMå“åº”é”™è¯¯: {e}")
            
        return response_message + "ğŸ˜Š"



